{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ggplot import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import textmining\n",
    "import csv\n",
    "from textblob import TextBlob\n",
    "import gc\n",
    "import os\n",
    "\n",
    "#nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "#Sci-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#visualization\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec \n",
    "\n",
    "color = sns.color_palette()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing train and test data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for the few rows\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['toxic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the percentae of train and test data sets\n",
    "nrow_train = train.shape[0]\n",
    "nrow_test = test.shape[0]\n",
    "sum = nrow_train + nrow_test\n",
    "\n",
    "print(\"            :Train      :Test\")\n",
    "print(\"Rows       :\",nrow_train,\":\",nrow_test)\n",
    "print(\"Percentage :\", round(nrow_train*100/sum),\"  :  \",round(nrow_test*100/sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class Imbalance\n",
    "x = train.iloc[:, 2:].sum()\n",
    "rowsum = train.iloc[:,2:].sum(axis = 1)\n",
    "train['Clean'] = (rowsum ==0)\n",
    "train['Clean'].sum()\n",
    "print(\"Total number of comments\", len(train))\n",
    "print(\"Total number of clean comments\", train['Clean'].sum())\n",
    "print(\"Total number of tags\", x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Total tags distribution\n",
    "x = train.iloc[:, 2:-1].sum()\n",
    "x1 = pd.DataFrame(x)\n",
    "x1 = x1.transpose()\n",
    "x1 = pd.melt(x1[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_imbalance = ggplot(x1, aes(x = 'variable', weight = 'value')) +\\\n",
    "geom_bar(fill = '#FF6666') + scale_y_continuous(labels='comma') + ggtitle('Number of tags per class')\n",
    "class_imbalance.save(filename = 'class_imbalance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multi-tagging\n",
    "Unique_sum = rowsum.unique()\n",
    "Total_unique = rowsum.nunique()\n",
    "total_value_count = rowsum.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_value_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x3 = pd.DataFrame(total_value_count)\n",
    "x3 = x3.iloc[1:,]\n",
    "x3 = x3.transpose()\n",
    "x3 = pd.melt(x3[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tags = ggplot(x3, aes(x = 'variable', weight = 'value')) +\\\n",
    "geom_bar(fill = \"#FF6666\") + scale_y_continuous(labels='comma') + ggtitle('Multiple tags per comment')\n",
    "multi_tags.save(filename = 'multitags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_all = train.ix[(train.identity_hate == 1) & (train.insult == 1) & (train.obscene ==1) & (train.severe_toxic == 1)\n",
    "                          & (train.toxic ==1) & (train.threat == 1)]\n",
    "train_hate = train.ix[(train.identity_hate == 1) & train.insult == 1)]\n",
    "train_toxic_i'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for missing value in train\n",
    "Missing_value = train.isnull().sum()\n",
    "#Checking for missing value in test\n",
    "Missing_value_test = train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection\n",
    "#Finding the correlation between the variables\n",
    "df_corr = train.iloc[:,2:-1]\n",
    "\n",
    "#Setting the plot height and width\n",
    "f, ax = plt.subplots(figsize=(9, 7))\n",
    "\n",
    "#Generating correlation matrix\n",
    "corr = df_corr.corr()\n",
    "\n",
    "#Ploting using seaborn library\n",
    "#annot shows the correlation number inside each block\n",
    "correlation_plot = sns.heatmap(corr, xticklabels = corr.columns.values, yticklabels = corr.columns.values, annot = True)\n",
    "#plt.show()\n",
    "plt.savefig(\"correlation_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.columns[0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visulizing the multitagging of toxic variable\n",
    "main_col = 'toxic'\n",
    "corr_mat1 = []\n",
    "for col in df_corr.columns[1:]:\n",
    "    confusion_matrix = pd.crosstab(df_corr[main_col], df_corr[col])\n",
    "    corr_mat1.append(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.concat(corr_mat1, axis = 1, keys = df_corr.columns[1:])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highlight_min(x):\n",
    "    is_min = x  == x.min()\n",
    "    return['background: yellow' if v else '' for v in is_min]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.style.apply(highlight_min, axis = 0)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_html('out.html')\n",
    "#subprocess.call(\n",
    " #   'wkhtmltoimage -f png --width 0 out.html out.png', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_col = 'insult'\n",
    "other_col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'identity_hate']\n",
    "corr_mat = []\n",
    "for col in other_col[0:]:\n",
    "    confusion_matrix = pd.crosstab(df_corr[main_col], df_corr[col])\n",
    "    corr_mat.append(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out1 = pd.concat(corr_mat, axis = 1, keys = other_col[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1.style.apply(highlight_min, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out1.to_html('out1.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_dataset = train.ix[(train.toxic == 1)]\n",
    "severetoxic_dataset = train.ix[(train.severe_toxic == 1)]\n",
    "obscene_dataset = train.ix[(train.obscene == 1)]\n",
    "threat_dataset = train.ix[(train.threat == 1)]\n",
    "identityhate_datset = train.ix[(train.identity_hate == 1)]\n",
    "insult_dataset = train.ix[(train.insult == 1)]\n",
    "clean_dataset = train.ix[(train.Clean == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Cloud\n",
    "wordcloud1 = WordCloud(width = 1000, height = 500, stopwords = STOPWORDS, background_color = 'black').generate(\n",
    "                        ''.join(clean_dataset.comment_text))\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.imshow(wordcloud1)\n",
    "plt.axis('off')\n",
    "plt.title('Wordcloud of Clean Comments')\n",
    "#plt.savefig('CleanCommentWC.png')\n",
    "#wordcloud1\n",
    "#correlation_plot.figure.savefig(\"correlation_plot.png\")\n",
    "\n",
    "#Word Cloud\n",
    "wordcloud = WordCloud(width = 1000, height = 500, stopwords = STOPWORDS, background_color = 'black').generate(\n",
    "                        ''.join(toxic_dataset.comment_text))\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('WordCloud of Toxic Comments')\n",
    "#plt.savefig('ToxicCommentWC.png')\n",
    "#plt.show()\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500, stopwords = STOPWORDS, background_color = 'black').generate(\n",
    "                        ''.join(severetoxic_dataset.comment_text))\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('WordCloud of Severe Toxic Comments')\n",
    "#plt.savefig('SevereToxicCommentWC.png')\n",
    "#plt.show()\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500, stopwords = STOPWORDS, background_color = 'black').generate(\n",
    "                        ''.join(obscene_dataset.comment_text))\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('WordCloud of Obscene Comments')\n",
    "#plt.savefig('ObsceneCommentWC.png')\n",
    "#plt.show()\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500, stopwords = STOPWORDS, background_color = 'black').generate(\n",
    "                        ''.join(threat_dataset.comment_text))\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('WordCloud of Threat Comments')\n",
    "#plt.savefig('ThreatCommentWC.png')\n",
    "#plt.show()\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500, stopwords = STOPWORDS, background_color = 'black').generate(\n",
    "                        ''.join(identityhate_datset.comment_text))\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('WordCloud of Identity Hate Comments')\n",
    "#plt.savefig('IHCommentWC.png')\n",
    "#plt.show()\n",
    "\n",
    "#Word Cloud\n",
    "wordcloud1 = WordCloud(width = 1000, height = 500, stopwords = STOPWORDS, background_color = 'black').generate(\n",
    "                        ''.join(insult_dataset.comment_text))\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.imshow(wordcloud1)\n",
    "plt.axis('off')\n",
    "plt.title('Wordcloud of Insult Comments')\n",
    "plt.savefig('All WordCloud.png')\n",
    "#wordcloud1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.comment_text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can use append as well as concat to add rows\n",
    "complete_data = pd.concat([train.iloc[:,0:2], test.iloc[:,0:2]])\n",
    "complete_data = complete_data.reset_index(drop = True)\n",
    "#Checking if the data is added sucessfully\n",
    "complete_data.shape[0] == train.shape[0] + test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Checking for indirect features\n",
    "1. Count of sentences\n",
    "2. Count of words\n",
    "3. Count of unique words\n",
    "4. Count of stopwords\n",
    "5. Count of letters\n",
    "6. Count of Punctuations\n",
    "7. Count of uppercase letters\n",
    "8. Avg length of words'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extracting stopwords\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "#extracting punctuations\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No of sentences in each document\n",
    "complete_data['count_sentence'] = complete_data['comment_text'].apply(lambda x: len(re.findall(\"\\n\", str(x))) + 1)\n",
    "#No of words in each document\n",
    "complete_data['count_words'] = complete_data['comment_text'].apply(lambda x: len(str(x).split()))\\\n",
    "#No of unique words in each document\n",
    "complete_data[\"count_uqwords\"] = complete_data['comment_text'].apply(lambda x: len(set(str(x).split())))\n",
    "#No of stopwrods in each document\n",
    "complete_data['count_stopwords'] = complete_data['comment_text'].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "#No of letters in each document\n",
    "complete_data['count_letters'] = complete_data['comment_text'].apply(lambda x: len(str(x)))\n",
    "#No of punctuation in each document\n",
    "complete_data['count_punctuations'] = complete_data['comment_text'].apply(lambda x: len([p for p in str(x) if p in string.punctuation]))\n",
    "#No of upercase letters in each document\n",
    "complete_data['count_uppercase'] = complete_data['comment_text'].apply(lambda x: len([u for u in str(x).split() if u.isupper()]))\n",
    "#Count of average length of words\n",
    "complete_data['avglen_words'] = complete_data['comment_text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Derived features\n",
    "complete_data['unqword_percent'] = complete_data['count_uqwords']*100/complete_data['count_words']\n",
    "complete_data['punct_percent'] = complete_data['count_punctuations']*100/complete_data['count_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_head = complete_data.head()\n",
    "feature_engineering_head.to_html('feature_engineering.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separating train and test\n",
    "train_new = complete_data.iloc[0:len(train),]\n",
    "test_new = complete_data.iloc[len(train):,]\n",
    "#Adding back the tags\n",
    "train_tags = train.iloc[:,2:]\n",
    "#adding back the tags to the new train data\n",
    "train_new = pd.concat([train_new, train_tags], axis = 1)\n",
    "train_new = train_new.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(train_new,aes(x = 'Clean', color = 'count_sentence')) +\\\n",
    "geom_bar() + scale_y_continuous(labels='comma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new.Clean = train_new.Clean.astype('category')\n",
    "train_new.severe_toxic = train_new.severe_toxic.astype('category')\n",
    "train_new.count_sentence = train_new.count_sentence.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(train_new, aes(x = 'Clean', y = 'count_sentence')) +\\\n",
    "    geom_violin() + theme_bw() + xlab(\"Clean\") + ylab(\"Count of Sentence\") +\\\n",
    "    ggtitle(\"Are long sentences more toxic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words with less than unique percent less than 30, calling as spammers\n",
    "spam = train_new.ix[(train_new.unqword_percent < 30)]\n",
    "spam_count = spam.iloc[:,12:18].sum(axis = 0)\n",
    "spam_count = pd.DataFrame(spam_count)\n",
    "spam_count = spam_count.transpose()\n",
    "spam_count = pd.melt(spam_count)\n",
    "spammer_plot = ggplot(spam_count, aes(x = 'variable', weight = 'value')) + geom_bar(fill= 'orange') + ggtitle(\"Comment with lower than (30%) unique words\") + theme_bw() \n",
    "spammer_plot.save(filename = 'SpammerPerClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spammers\n",
    "spam.comment_text[4:]\n",
    "spam[spam.identity_hate ==1].comment_text.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam[spam.toxic ==1].comment_text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam[spam.severe_toxic ==1].comment_text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam[spam.obscene ==1].comment_text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam[spam.insult ==1].comment_text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam[spam.threat ==1].comment_text.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new.comment_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_new.Clean.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting tags into category type\n",
    "for i in np.arange(12,19):\n",
    "    train_new.iloc[:,i] = train_new.iloc[:,i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#***********Corpus Cleaning*******************\n",
    "corpus = complete_data.comment_text\n",
    "#Punctuation Marks\n",
    "#Numbers\n",
    "#Case folding\n",
    "#Stop words\n",
    "#White spaces\n",
    "#Stemming\n",
    "#Lemmatization\n",
    "#Synonym check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extracting stop words\n",
    "#train_clean.comment_text = str(train_clean.comment_text)\n",
    "#train_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in eng_stopwords])\n",
    "    punc_free = ''.join([punc for punc in stop_free if punc not in exclude])\n",
    "    num_free = ''.join([i for i in punc_free if not i.isdigit()])\n",
    "    return num_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpus.apply(lambda x :clean(x))\n",
    "#clean_corpus = [clean(train_clean.iloc[i,1])for i in range(0, train_clean.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_corpus = list(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new.comment_text[159524]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.iloc[159524]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term document Matrix\n",
    "'''tdm = textmining.TermDocumentMatrix()\n",
    "\n",
    "for i in clean_corpus:\n",
    "    tdm.add_doc(i)\n",
    "\n",
    "#Calculating tf-idf score\n",
    "#v = TfidfVectorizer()\n",
    "#x = v.fit_transform(clean_corpus)\n",
    "\n",
    "#some detailed description of the parameters\n",
    "# min_df=10 --- ignore terms that appear lesser than 10 times \n",
    "# max_features=None  --- Create as many words as present in the text corpus\n",
    "\n",
    "#Dont run\n",
    "vec = CountVectorizer(min_df=200,  max_features=10000)\n",
    "X = vec.fit_transform(train_clean.comment_text)\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "#causes memory issues when using min_df = 10'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=200,  max_features=10000, \n",
    "            strip_accents='unicode', analyzer='word',ngram_range=(1,1),\n",
    "            use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "            stop_words = 'english')\n",
    "tfv.fit(corpus)\n",
    "features = np.array(tfv.get_feature_names())\n",
    "train_unigrams =  tfv.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separating the train and test corpus\n",
    "train_unigrams =  tfv.transform(corpus.iloc[:train.shape[0]])\n",
    "test_unigrams = tfv.transform(corpus.iloc[train.shape[0]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Top tf-idf features\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    \n",
    "    D = Xtr[grp_ids].toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "# modified for multilabel milticlass\n",
    "def top_feats_by_class(Xtr, features, min_tfidf=0.1, top_n=20):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    cols=train_tags.columns\n",
    "    for col in cols:\n",
    "        ids = train_tags.index[train_tags[col]==1]\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        #feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_top_n_per_lass = top_feats_by_class(train_unigrams,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,22))\n",
    "plt.suptitle(\"TF_IDF Top words per class(unigrams)\",fontsize=20)\n",
    "gridspec.GridSpec(4,2)\n",
    "plt.subplot2grid((4,2),(0,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[0].feature.iloc[0:9],tfidf_top_n_per_lass[0].tfidf.iloc[0:9],color= color[0])\n",
    "plt.title(\"class : Toxic\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "plt.savefig('Toxic_TF-IDFScore.png')\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(0,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[1].feature.iloc[0:9],tfidf_top_n_per_lass[1].tfidf.iloc[0:9],color=color[1])\n",
    "plt.title(\"class : Severe toxic\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "plt.savefig('SevereToxic_TF-IDFScore.png')\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(1,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[2].feature.iloc[0:9],tfidf_top_n_per_lass[2].tfidf.iloc[0:9],color=color[2])\n",
    "plt.title(\"class : Obscene\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "plt.savefig('Obscene_TF-IDFScore.png')\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(1,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[3].feature.iloc[0:9],tfidf_top_n_per_lass[3].tfidf.iloc[0:9],color=color[3])\n",
    "plt.title(\"class : Threat\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "plt.savefig('Threat_TF-IDFScore.png')\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(2,0))\n",
    "sns.barplot(tfidf_top_n_per_lass[4].feature.iloc[0:9],tfidf_top_n_per_lass[4].tfidf.iloc[0:9],color=color[4])\n",
    "plt.title(\"class : Insult\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "plt.savefig('Insult_TF-IDFScore.png')\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(2,1))\n",
    "sns.barplot(tfidf_top_n_per_lass[5].feature.iloc[0:9],tfidf_top_n_per_lass[5].tfidf.iloc[0:9],color=color[5])\n",
    "plt.title(\"class : Identity hate\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "plt.savefig('Identity_hate_TF-IDFScore.png')\n",
    "\n",
    "\n",
    "plt.subplot2grid((4,2),(3,0),colspan=2)\n",
    "sns.barplot(tfidf_top_n_per_lass[6].feature.iloc[0:19],tfidf_top_n_per_lass[6].tfidf.iloc[0:19])\n",
    "plt.title(\"class : Clean\",fontsize=15)\n",
    "plt.xlabel('Word', fontsize=12)\n",
    "plt.ylabel('TF-IDF score', fontsize=12)\n",
    "plt.savefig('Clean_TF-IDFScore.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n",
    "y = train[target_col]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean.TF_IDFSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_corpus.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model \n",
    "prd = np.zeros((test_unigrams.shape[0],y.shape[1]))\n",
    "cv_score =[]\n",
    "for i,col in enumerate(target_col):\n",
    "    lr = LogisticRegression(C=2,random_state = i,class_weight = 'balanced')\n",
    "    print('Building {} model for column:{''}'.format(i,col)) \n",
    "    lr.fit(train_unigrams,y[col])\n",
    "    #print(result.summary())\n",
    "    cv_score.append(lr.score)\n",
    "    prd[:,i] = lr.predict_proba(test_unigrams)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Validation on train data set\n",
    "col = 'identity_hate'\n",
    "print(\"Column:\",col)\n",
    "pred =  lr.predict(train_unigrams)\n",
    "cof_ih = confusion_matrix(y[col],pred)\n",
    "clas_ih = classification_report(y[col],pred)\n",
    "#cof_ih.to_html('cof_ih.html')\n",
    "#clas_ih.to_html('clas_ih.html')\n",
    "print('\\nConfusion matrix\\n',confusion_matrix(y[col],pred))\n",
    "print(classification_report(y[col],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.concat([pd.DataFrame(features),pd.DataFrame(np.transpose(lr.coef_))], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Validation on train data set\n",
    "col = 'toxic'\n",
    "print(\"Column:\",col)\n",
    "pred =  lr.predict(train_unigrams)\n",
    "cof_ih = confusion_matrix(y[col],pred)\n",
    "clas_ih = classification_report(y[col],pred)\n",
    "#cof_ih.to_html('cof_ih.html')\n",
    "#clas_ih.to_html('clas_ih.html')\n",
    "print('\\nConfusion matrix\\n',confusion_matrix(y[col],pred))\n",
    "print(classification_report(y[col],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefHead = coefficients.head(15)\n",
    "coefHead.to_html('CoefficientHead.html')\n",
    "toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Validation on train data set\n",
    "col = 'severe_toxic'\n",
    "print(\"Column:\",col)\n",
    "pred =  lr.predict(train_unigrams)\n",
    "cof_ih = confusion_matrix(y[col],pred)\n",
    "clas_ih = classification_report(y[col],pred)\n",
    "#cof_ih.to_html('cof_ih.html')\n",
    "#clas_ih.to_html('clas_ih.html')\n",
    "print('\\nConfusion matrix\\n',confusion_matrix(y[col],pred))\n",
    "print(classification_report(y[col],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Validation on train data set\n",
    "col = 'obscene'\n",
    "print(\"Column:\",col)\n",
    "pred =  lr.predict(train_unigrams)\n",
    "cof_ih = confusion_matrix(y[col],pred)\n",
    "clas_ih = classification_report(y[col],pred)\n",
    "#cof_ih.to_html('cof_ih.html')\n",
    "#clas_ih.to_html('clas_ih.html')\n",
    "print('\\nConfusion matrix\\n',confusion_matrix(y[col],pred))\n",
    "print(classification_report(y[col],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Validation on train data set\n",
    "col = 'threat'\n",
    "print(\"Column:\",col)\n",
    "pred =  lr.predict(train_unigrams)\n",
    "cof_ih = confusion_matrix(y[col],pred)\n",
    "clas_ih = classification_report(y[col],pred)\n",
    "#cof_ih.to_html('cof_ih.html')\n",
    "#clas_ih.to_html('clas_ih.html')\n",
    "print('\\nConfusion matrix\\n',confusion_matrix(y[col],pred))\n",
    "print(classification_report(y[col],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Validation on train data set\n",
    "col = 'insult'\n",
    "print(\"Column:\",col)\n",
    "pred =  lr.predict(train_unigrams)\n",
    "cof_ih = confusion_matrix(y[col],pred)\n",
    "clas_ih = classification_report(y[col],pred)\n",
    "#cof_ih.to_html('cof_ih.html')\n",
    "#clas_ih.to_html('clas_ih.html')\n",
    "print('\\nConfusion matrix\\n',confusion_matrix(y[col],pred))\n",
    "print(classification_report(y[col],pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission\n",
    "prd_1 = pd.DataFrame(prd,columns=y.columns)\n",
    "submit = pd.concat([test['id'],prd_1],axis=1)\n",
    "#submit.to_csv('toxic_lr.csv',index=False)\n",
    "#submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = submit.head(10)\n",
    "head.to_html('submit.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
